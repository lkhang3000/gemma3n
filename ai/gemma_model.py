import requests
import json
import os
import subprocess
import time
from pathlib import Path

class OllamaGemmaModel:
    def __init__(self):
        self.base_url = "http://localhost:11434"
        self.model_name = "gemma2:2b"  # hoáº·c gemma:2b
        self.is_loaded = False
        self.ollama_running = False
        
        print("ğŸ¦™ Khá»Ÿi táº¡o Ollama Gemma Model")
        
    def check_ollama_installed(self):
        """Kiá»ƒm tra Ollama Ä‘Ã£ cÃ i Ä‘áº·t chÆ°a"""
        try:
            result = subprocess.run(
                ["ollama", "--version"], 
                capture_output=True, 
                text=True, 
                timeout=5
            )
            if result.returncode == 0:
                print(f"âœ… Ollama Ä‘Ã£ cÃ i Ä‘áº·t: {result.stdout.strip()}")
                return True
            else:
                print("âŒ Ollama chÆ°a cÃ i Ä‘áº·t")
                return False
        except (subprocess.TimeoutExpired, FileNotFoundError):
            print("âŒ Ollama chÆ°a cÃ i Ä‘áº·t hoáº·c khÃ´ng trong PATH")
            return False
    
    def install_ollama_guide(self):
        """HÆ°á»›ng dáº«n cÃ i Ä‘áº·t Ollama"""
        print("\nğŸ“‹ HÆ¯á»šNG DáºªN CÃ€I Äáº¶T OLLAMA:")
        print("=" * 40)
        print("ğŸŒ Windows:")
        print("   1. Táº£i tá»«: https://ollama.com/download")
        print("   2. Cháº¡y file .exe vÃ  lÃ m theo hÆ°á»›ng dáº«n")
        print("   3. Restart mÃ¡y tÃ­nh")
        print("\nğŸ§ Linux/Mac:")
        print("   curl -fsSL https://ollama.com/install.sh | sh")
        print("\nâš ï¸ Sau khi cÃ i Ä‘áº·t, cháº¡y láº¡i script nÃ y")
        
    def start_ollama_service(self):
        """Khá»Ÿi Ä‘á»™ng Ollama service"""
        print("ğŸš€ Äang khá»Ÿi Ä‘á»™ng Ollama service...")
        
        try:
            # Thá»­ káº¿t ná»‘i trÆ°á»›c
            response = requests.get(f"{self.base_url}/api/tags", timeout=5)
            if response.status_code == 200:
                print("âœ… Ollama service Ä‘Ã£ cháº¡y")
                self.ollama_running = True
                return True
        except requests.exceptions.RequestException:
            pass
        
        # Náº¿u chÆ°a cháº¡y, thá»­ khá»Ÿi Ä‘á»™ng
        try:
            if os.name == 'nt':  # Windows
                subprocess.Popen(["ollama", "serve"], shell=True)
            else:  # Linux/Mac
                subprocess.Popen(["ollama", "serve"])
            
            print("â³ Äang chá» Ollama service khá»Ÿi Ä‘á»™ng...")
            time.sleep(3)
            
            # Kiá»ƒm tra láº¡i
            for i in range(10):
                try:
                    response = requests.get(f"{self.base_url}/api/tags", timeout=2)
                    if response.status_code == 200:
                        print("âœ… Ollama service Ä‘Ã£ sáºµn sÃ ng")
                        self.ollama_running = True
                        return True
                except requests.exceptions.RequestException:
                    time.sleep(2)
                    print(f"â³ Thá»­ láº¡i... ({i+1}/10)")
            
            print("âŒ KhÃ´ng thá»ƒ khá»Ÿi Ä‘á»™ng Ollama service")
            return False
            
        except Exception as e:
            print(f"âŒ Lá»—i khá»Ÿi Ä‘á»™ng Ollama: {e}")
            return False
    
    def list_available_models(self):
        """Liá»‡t kÃª cÃ¡c model cÃ³ sáºµn"""
        try:
            response = requests.get(f"{self.base_url}/api/tags")
            if response.status_code == 200:
                models = response.json().get("models", [])
                print(f"ğŸ“‹ CÃ³ {len(models)} model Ä‘Ã£ cÃ i Ä‘áº·t:")
                for model in models:
                    name = model.get("name", "Unknown")
                    size = model.get("size", 0) / (1024**3)  # GB
                    print(f"   ğŸ¤– {name} ({size:.1f} GB)")
                return [model.get("name") for model in models]
            else:
                print("âŒ KhÃ´ng thá»ƒ láº¥y danh sÃ¡ch model")
                return []
        except Exception as e:
            print(f"âŒ Lá»—i láº¥y danh sÃ¡ch model: {e}")
            return []
    
    def pull_model(self, model_name=None):
        """Táº£i model tá»« Ollama registry"""
        if not model_name:
            model_name = self.model_name
            
        print(f"ğŸ“¥ Äang táº£i model {model_name}...")
        print("â³ QuÃ¡ trÃ¬nh nÃ y cÃ³ thá»ƒ máº¥t vÃ i phÃºt...")
        
        try:
            # Stream response Ä‘á»ƒ hiá»ƒn thá»‹ progress
            response = requests.post(
                f"{self.base_url}/api/pull",
                json={"name": model_name},
                stream=True
            )
            
            if response.status_code == 200:
                for line in response.iter_lines():
                    if line:
                        try:
                            data = json.loads(line)
                            if "status" in data:
                                status = data["status"]
                                if "completed" in data and "total" in data:
                                    completed = data["completed"]
                                    total = data["total"]
                                    percent = (completed / total) * 100
                                    print(f"\rğŸ“Š {status}: {percent:.1f}%", end="", flush=True)
                                else:
                                    print(f"\rğŸ”„ {status}", end="", flush=True)
                        except json.JSONDecodeError:
                            continue
                
                print("\nâœ… Model Ä‘Ã£ táº£i xong!")
                return True
            else:
                print(f"âŒ Lá»—i táº£i model: {response.status_code}")
                return False
                
        except Exception as e:
            print(f"âŒ Lá»—i táº£i model: {e}")
            return False
    
    def load_model(self):
        """Táº£i vÃ  chuáº©n bá»‹ model"""
        if self.is_loaded:
            return True
        
        print("ğŸ” Äang kiá»ƒm tra Ollama...")
        
        # 1. Kiá»ƒm tra Ollama Ä‘Ã£ cÃ i Ä‘áº·t
        if not self.check_ollama_installed():
            self.install_ollama_guide()
            return False
        
        # 2. Khá»Ÿi Ä‘á»™ng Ollama service
        if not self.start_ollama_service():
            return False
        
        # 3. Kiá»ƒm tra model Ä‘Ã£ cÃ³ chÆ°a
        available_models = self.list_available_models()
        model_exists = any(self.model_name in model for model in available_models)
        
        if not model_exists:
            print(f"ğŸ“¥ Model {self.model_name} chÆ°a cÃ³, Ä‘ang táº£i...")
            
            # Thá»­ cÃ¡c tÃªn model khÃ¡c nhau
            model_variants = [
                "gemma2:2b",
                "gemma:2b", 
                "gemma2:2b-instruct",
                "gemma:2b-instruct"
            ]
            
            success = False
            for variant in model_variants:
                print(f"ğŸ”„ Thá»­ táº£i {variant}...")
                if self.pull_model(variant):
                    self.model_name = variant
                    success = True
                    break
            
            if not success:
                print("âŒ KhÃ´ng thá»ƒ táº£i model Gemma")
                print("ğŸ’¡ CÃ¡c model kháº£ dá»¥ng khÃ¡c:")
                print("   - llama3.2:1b (nháº¹)")
                print("   - llama3.2:3b (trung bÃ¬nh)")
                print("   - qwen2.5:1.5b (nháº¹)")
                return False
        
        # 4. Test model
        print(f"ğŸ§ª Äang test model {self.model_name}...")
        test_response = self.get_response("Hello", max_length=50)
        if test_response and "lá»—i" not in test_response.lower():
            print("âœ… Model Ä‘Ã£ sáºµn sÃ ng!")
            self.is_loaded = True
            return True
        else:
            print("âŒ Model khÃ´ng hoáº¡t Ä‘á»™ng Ä‘Ãºng")
            return False
    
    def get_response(self, user_message, max_length=200, temperature=0.7):
        """Táº¡o response tá»« user message"""
        if not self.is_loaded:
            if not self.load_model():
                return "âŒ Model chÆ°a sáºµn sÃ ng"
        
        try:
            # Táº¡o prompt
            prompt = self.create_prompt(user_message)
            
            # Gá»i Ollama API
            payload = {
                "model": self.model_name,
                "prompt": prompt,
                "stream": False,
                "options": {
                    "temperature": temperature,
                    "num_predict": max_length,
                    "top_p": 0.9,
                    "top_k": 40,
                    "repeat_penalty": 1.1
                }
            }
            
            response = requests.post(
                f"{self.base_url}/api/generate",
                json=payload,
                timeout=30
            )
            
            if response.status_code == 200:
                result = response.json()
                bot_response = result.get("response", "").strip()
                
                # Xá»­ lÃ½ response
                bot_response = self.clean_response(bot_response)
                
                return bot_response or "Xin lá»—i, tÃ´i khÃ´ng thá»ƒ táº¡o pháº£n há»“i."
            else:
                return f"âŒ Lá»—i API: {response.status_code}"
                
        except requests.exceptions.Timeout:
            return "â° Timeout - Model máº¥t quÃ¡ nhiá»u thá»i gian"
        except Exception as e:
            return f"âŒ Lá»—i: {str(e)}"
    
    def create_prompt(self, user_message):
        """Táº¡o prompt cho Gemma"""
        # Prompt Ä‘Æ¡n giáº£n cho Ollama
        return f"""<start_of_turn>user
{user_message}<end_of_turn>
<start_of_turn>model
"""
    
    def clean_response(self, response):
        """LÃ m sáº¡ch response"""
        # Loáº¡i bá» cÃ¡c tag Ä‘áº·c biá»‡t
        response = response.replace("<start_of_turn>", "")
        response = response.replace("<end_of_turn>", "")
        response = response.replace("model", "").strip()
        
        # Loáº¡i bá» xuá»‘ng dÃ²ng thá»«a
        response = response.replace("\n\n", "\n").strip()
        
        return response
    
    def get_model_info(self):
        """Láº¥y thÃ´ng tin model"""
        info = {
            "model_name": self.model_name,
            "base_url": self.base_url,
            "is_loaded": self.is_loaded,
            "ollama_running": self.ollama_running
        }
        
        # ThÃªm thÃ´ng tin system
        try:
            response = requests.get(f"{self.base_url}/api/tags")
            if response.status_code == 200:
                models = response.json().get("models", [])
                info["available_models"] = len(models)
        except:
            pass
        
        return info
    
    def cleanup(self):
        """Dá»n dáº¹p (khÃ´ng cáº§n thiáº¿t cho Ollama)"""
        print("ğŸ§¹ Ollama model cleanup hoÃ n táº¥t")
        self.is_loaded = False

# Test function
if __name__ == "__main__":
    print("ğŸ§ª Testing Ollama Gemma Model...")
    
    model = OllamaGemmaModel()
    
    try:
        if model.load_model():
            # Test conversation
            test_messages = [
                "Xin chÃ o!",
                "Báº¡n cÃ³ thá»ƒ giÃºp tÃ´i khÃ´ng?",
                "HÃ´m nay thá»i tiáº¿t tháº¿ nÃ o?"
            ]
            
            for msg in test_messages:
                print(f"\nğŸ‘¤ User: {msg}")
                response = model.get_response(msg)
                print(f"ğŸ¤– Bot: {response}")
        else:
            print("âŒ KhÃ´ng thá»ƒ táº£i model")
            
    except KeyboardInterrupt:
        print("\nğŸ‘‹ Test bá»‹ há»§y")
    except Exception as e:
        print(f"âŒ Test failed: {e}")
    finally:
        model.cleanup()